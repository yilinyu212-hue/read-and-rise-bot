import os, feedparser, requests, json
from datetime import datetime
from notion_client import Client

# é…ç½®è¯»å–
DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY")
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
DATABASE_ID = os.environ.get("DATABASE_ID")

notion = Client(auth=NOTION_TOKEN)

def get_ai_deep_notes(title):
    if not DEEPSEEK_KEY: return "AI å¯†é’¥æœªé…ç½®"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {DEEPSEEK_KEY}"}
    
    # æ›´åŠ æ·±åº¦ã€ä¸“ä¸šçš„æ•™ç ” Prompt
    prompt = f"""
    ä½œä¸ºå¤–åˆŠç²¾è¯»ä¸“å®¶ï¼Œè¯·é’ˆå¯¹ã€Š{title}ã€‹åˆ¶ä½œä¸€ä»½æ·±åº¦æ•™ç ”ç¬”è®°ï¼š
    
    1. ğŸ’¡ã€æ ¸å¿ƒçºµè§ˆã€‘ï¼šç”¨3è¡Œä»¥å†…æ–‡å­—è§£ææ–‡ç« çš„ç¤¾ä¼šèƒŒæ™¯ä¸æ ¸å¿ƒäº‰è®®ç‚¹ã€‚
    2. ğŸ“ã€ç²¾è¯»ç¬”è®°ã€‘ï¼š
       - é€»è¾‘æ‹†è§£ï¼šç®€è¿°æ–‡ç« æ˜¯å¦‚ä½•å±•å¼€è®ºè¿°çš„ï¼ˆStart -> Develop -> Endï¼‰ã€‚
       - æ·±åº¦è§è§£ï¼šæŒ–æ˜æ–‡ä¸­ä¸€ä¸ªå®¹æ˜“è¢«å¿½è§†çš„ç»†èŠ‚æˆ–æ·±åº¦å«ä¹‰ã€‚
    3. ğŸ¯ã€åœ°é“è¡¨è¾¾ã€‘ï¼š
       - æå–2ä¸ªé«˜é˜¶è¯ç»„ï¼ˆå«æ­é…ã€ä¸­è‹±æ–‡å¯¹ç…§åŠä¾‹å¥ï¼‰ã€‚
       - æå–1ä¸ªé•¿éš¾å¥ï¼Œè¿›è¡Œè¯­æ³•ç»“æ„æ‹†è§£ï¼ˆå¦‚å®šè¯­ä»å¥ã€å€’è£…ç­‰ï¼‰ã€‚
    4. âœï¸ã€å†™ä½œ/å£è¯­å€Ÿé‰´ã€‘ï¼š
       - æç‚¼ä¸€ä¸ªæ–‡ä¸­çš„é€»è¾‘è¡”æ¥å¥å¼ã€‚
       - é’ˆå¯¹â€œæ•™è‚²ç­–å±•â€æˆ–â€œæ•™å­¦åœºæ™¯â€æä¾›ä¸€ä¸ªä»¿å†™ä¾‹å¥ã€‚
    
    è¦æ±‚ï¼šé€»è¾‘ä¸¥å¯†ï¼Œè¯­è¨€ä¸“ä¸šä¸”å¯Œæœ‰å¯å‘æ€§ï¼Œä½¿ç”¨ Markdown æ ¼å¼æ’ç‰ˆã€‚
    """
    
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„é¡¶çº§å¤–åˆŠç²¾è¯»æ•™ç»ƒï¼Œæ“…é•¿æ·±åº¦é€»è¾‘åˆ†æå’Œè¯­è¨€ç‚¹æŒ–æ˜ã€‚"},
            {"role": "user", "content": prompt}
        ]
    }
    try:
        res = requests.post("https://api.deepseek.com/chat/completions", headers=headers, json=data, timeout=60)
        return res.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"ç¬”è®°ç”Ÿæˆä¸­ï¼Œæš‚é‡å°é—®é¢˜: {e}"

def run():
    SOURCES = [
        {"name": "Economist", "url": "https://www.economist.com/briefing/rss.xml"},
        {"name": "NYT", "url": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml"}
    ]
    
    all_articles = []
    print("ğŸš€ å¼€å§‹ç”Ÿæˆæ·±åº¦ç²¾è¯»ç¬”è®°...")
    
    for src in SOURCES:
        feed = feedparser.parse(src['url'])
        for entry in feed.entries[:2]:
            print(f"ğŸ“˜ æ­£åœ¨ç ”è¯»: {entry.title}")
            notes = get_ai_deep_notes(entry.title)
            
            # åŒæ­¥åˆ° Notion
            try:
                notion.pages.create(
                    parent={"database_id": DATABASE_ID},
                    properties={
                        "Name": {"title": [{"text": {"content": entry.title}}]},
                        "Source": {"select": {"name": src['name']}},
                        "Link": {"url": entry.link},
                        "AI Summary": {"rich_text": [{"text": {"content": notes[:1950]}}]}, # Notion å•å…ƒæ ¼ä¸Šé™çº¦2000å­—ç¬¦
                        "Date": {"date": {"start": datetime.now().strftime("%Y-%m-%d")}},
                        "Status": {"status": {"name": "To Read"}}
                    }
                )
            except Exception as e:
                print(f"âŒ Notion åŒæ­¥å¤±è´¥: {e}")

            all_articles.append({
                "source": src['name'],
                "title": entry.title,
                "content": notes, # ç½‘ç«™ç«¯æ˜¾ç¤ºå®Œæ•´çš„ç¬”è®°
                "date": datetime.now().strftime("%Y-%m-%d")
            })

    # æ›´æ–°æœ¬åœ°æ•°æ®
    os.makedirs('data', exist_ok=True)
    with open('data/library.json', 'w', encoding='utf-8') as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    run()
