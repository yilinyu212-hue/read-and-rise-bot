import requests, feedparser, json, os, time
from datetime import datetime

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

RSS_SOURCES = [
    {"name": "HBR", "url": "https://hbr.org/rss/feed/topics/leadership"},
    {"name": "McKinsey", "url": "https://www.mckinsey.com/insights/rss"},
    {"name": "The Economist", "url": "https://www.economist.com/business/rss.xml"},
    {"name": "Fortune", "url": "https://fortune.com/feed/"}
]

def ai_call(prompt, is_json=True):
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½éº¦è‚¯é”¡èƒŒæ™¯çš„ç²¾è‹±æ•™ç»ƒã€‚"}, {"role": "user", "content": prompt}],
        "temperature": 0.3
    }
    if is_json: payload["response_format"] = {"type": "json_object"}
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=60)
        content = res.json()['choices'][0]['message']['content']
        return json.loads(content) if is_json else content
    except: return None

def run_sync():
    # åˆå§‹åŒ–ç»“æ„ï¼Œç¡®ä¿å­—æ®µå®Œæ•´
    data = {
        "briefs": [], 
        "deep_articles": [], 
        "weekly_question": {"cn": "å¦‚ä½•åˆ©ç”¨ç¬¬ä¸€æ€§åŸç†é‡æ„ç«äº‰åŠ›ï¼Ÿ", "en": "How to leverage First Principles to rebuild competitiveness?"},
        "update_time": datetime.now().strftime("%Y-%m-%d %H:%M")
    }
    
    # å¦‚æœå·²æœ‰æ•°æ®åˆ™è¯»å–ï¼Œä¿ç•™ deep_articles
    if os.path.exists("data.json"):
        try:
            with open("data.json", "r", encoding="utf-8") as f:
                old_data = json.load(f)
                data["deep_articles"] = old_data.get("deep_articles", [])
                # å¦‚æœæ—§æ•°æ®æœ‰æé—®ä¹Ÿä¿ç•™
                if "weekly_question" in old_data:
                    data["weekly_question"] = old_data["weekly_question"]
        except: pass

    print("ğŸ“¡ æ­£åœ¨æŠ“å–å¿«æŠ¥...")
    for s in RSS_SOURCES:
        try:
            feed = feedparser.parse(s['url'])
            for item in feed.entries[:1]:
                data["briefs"].append({
                    "title": item.title, 
                    "link": item.link, 
                    "source": s['name'],
                    "time": datetime.now().strftime("%m-%d")
                })
        except: continue

    with open("data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print("âœ… åŒæ­¥å®Œæˆ")

if __name__ == "__main__":
    run_sync()
