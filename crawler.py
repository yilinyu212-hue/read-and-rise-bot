import os, feedparser, json, requests
from datetime import datetime
from notion_client import Client

# è¯»å–é…ç½®
DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY")
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
DATABASE_ID = os.environ.get("DATABASE_ID")

notion = Client(auth=NOTION_TOKEN)

def get_ai_analysis(title):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_KEY}"
    }
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½Read & Riseæ•™è‚²ç­–å±•äººã€‚è¯·ç”¨ä¸­æ–‡æ€»ç»“æ ¸å¿ƒè§‚ç‚¹ã€æ ‡æ³¨éš¾åº¦å¹¶æå–3ä¸ªé‡ç‚¹è¯æ±‡ã€‚"},
            {"role": "user", "content": f"æ–‡ç« æ ‡é¢˜: {title}"}
        ]
    }
    try:
        res = requests.post("https://api.deepseek.com/chat/completions", headers=headers, json=data, timeout=30)
        return res.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"è§£æç”Ÿæˆä¸­... (API é”™è¯¯: {e})"

def push_to_notion(title, link, content):
    # ã€å¼ºåˆ¶æ‰“å°ã€‘çœ‹çœ‹æœºå™¨äººåˆ°åº•æ‹¿åˆ°äº†ä»€ä¹ˆï¼ˆå®‰å…¨èµ·è§åªæ‰“é•¿åº¦ï¼‰
    print(f"DEBUG: æ­£åœ¨å°è¯•è¿æ¥ Notion... Token é•¿åº¦: {len(str(NOTION_TOKEN))}, ID é•¿åº¦: {len(str(DATABASE_ID))}")
    
    try:
        notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties={
                "Name": {"title": [{"text": {"content": title}}]},
                "Source": {"select": {"name": "Economist"}}, 
                "Link": {"url": link},                       
                "AI Summary": {"rich_text": [{"text": {"content": content[:1900]}}]}, 
                "Date": {"date": {"start": datetime.now().strftime("%Y-%m-%d")}},
                "Status": {"status": {"name": "To Read"}}
            }
        )
        print(f"ğŸš€ ç»ˆäºæˆåŠŸäº†ï¼æ•°æ®å·²è¿›å…¥ Notion çœ‹æ¿ï¼")
    except Exception as e:
        print(f"âŒ å…³é”®æŠ¥é”™ï¼šNotion æœåŠ¡å™¨æ‹’ç»äº†è¯·æ±‚ã€‚åŸå› : {e}")

def run():
    feed = feedparser.parse("https://www.economist.com/briefing/rss.xml")
    articles = []
    
    for entry in feed.entries[:3]:
        print(f"æ­£åœ¨å¤„ç†: {entry.title}")
        analysis = get_ai_analysis(entry.title)
        articles.append({"title": entry.title, "link": entry.link, "content": analysis, "date": datetime.now().strftime("%Y-%m-%d")})
        
        # åˆ æ‰äº†åŸæ¥çš„ if åˆ¤æ–­ï¼Œå¼ºè¡Œå°è¯•æ¨é€
        push_to_notion(entry.title, entry.link, analysis)

    os.makedirs('data', exist_ok=True)
    with open('data/library.json', 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    run()
